{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will find and deploy the most affordable GPU's on the market with the Shadeform API, deploy a model serving framework called VLLM to serve Mistral. We limit our search to machines with 1xA6000 machines, but that is easily configurable below.\n",
    "\n",
    "This Notebook re-uses code from [find_and_use_gpus.ipynb](https://github.com/shadeform/examples/blob/main/find_and_use_gpus.ipynb) for using the Shadeform API to find available instances, and leverages code from [deploy_container.ipynb](https://github.com/shadeform/examples/blob/main/deploy_container.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://api.shadeform.ai/v1/instances\"\n",
    "instance_type_url = base_url + \"/types\"\n",
    "create_url = base_url + \"/create\"\n",
    "headers = {\n",
    "    \"X-API-KEY\": \"<Add-your-key-here>\", \n",
    "    \"Content-Type\" : \"application/json\"\n",
    "}\n",
    "shade_instance_type = \"A6000\"\n",
    "gpu_type = \"A6000\"\n",
    "num_gpus = 1\n",
    "\n",
    "params = {\n",
    "    'gpu_type' : gpu_type,\n",
    "    'sort' : 'price',\n",
    "    'available' : True,\n",
    "    'num_gpus' : num_gpus\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", instance_type_url, headers=headers, params=params)\n",
    "instance_types = json.loads(response.text)[\"instance_types\"]\n",
    "best_instance = None\n",
    "region = None\n",
    "if len(instance_types) > 0:\n",
    "    best_instance = instance_types[0]\n",
    "    region = best_instance['availability'][0]['region']\n",
    "    print(f\"The cheapest {gpu_type} instance with {num_gpus} gpu(s) is:\", best_instance)\n",
    "else:\n",
    "    print(f\"No instances of type {gpu_type} instance with {num_gpus} gpu(s) found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the model you need requires authenticated access, paste your key here\n",
    "huggingface_token = \"\"\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "payload = {\n",
    "  \"cloud\": best_instance[\"cloud\"],\n",
    "  \"region\": region,\n",
    "  \"shade_instance_type\": shade_instance_type,\n",
    "  \"shade_cloud\": True,\n",
    "  \"name\": \"cool_gpu_server\",\n",
    "  \"launch_configuration\": {\n",
    "    \"type\": \"docker\",\n",
    "     #This selects the image to launch, and sets environment variables \"tasks\" and \"num_fewshot\"\n",
    "    \"docker_configuration\": {\n",
    "      \"image\": \"vllm/vllm-openai:latest\",\n",
    "      \"args\": \"--model \" + model_id,\n",
    "      \"envs\": [],\n",
    "      \"port_mappings\": [\n",
    "        {\n",
    "          \"container_port\": 8000,\n",
    "          \"host_port\": 8000\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "#Add another environment variable to the payload by adding a json\n",
    "if huggingface_token != \"\":\n",
    "  token_env_json = {\n",
    "    \"name\": \"HUGGING_FACE_HUB_TOKEN\",\n",
    "    \"value\" : huggingface_token\n",
    "  }\n",
    "  payload[\"launch_configuration\"][\"docker_configuration\"][\"envs\"].append(token_env_json)\n",
    "\n",
    "print(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#request the best instance that is available\n",
    "response = requests.request(\"POST\", create_url, json=payload, headers=headers)\n",
    "#easy way to visually see if this request worked\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_response = requests.request(\"GET\", base_url, headers=headers)\n",
    "ip_addr = \"\"\n",
    "print(instance_response.text)\n",
    "instance = json.loads(instance_response.text)[\"instances\"][0]\n",
    "instance_status = instance['status']\n",
    "if instance_status == 'active':\n",
    "    print(f\"Instance is active with IP: {instance['ip']}\")\n",
    "    ip_addr = instance['ip']\n",
    "else:\n",
    "    print(f\"Instance isn't yet active: {instance}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wait until the previous cell has an IP address associated with it, and then add a few minutes for the VLLM server to stand up. \n",
    "#It is usually best to look at the logs on the dashboard to tell when the model is loaded.\n",
    "\n",
    "model_list_response = requests.get(f'http://{ip_addr}:8000/v1/models')\n",
    "print(model_list_response.text)\n",
    "\n",
    "vllm_headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "\n",
    "json_data = {\n",
    "    'model': model_id,\n",
    "    'prompt': 'San Francisco is a',\n",
    "    'max_tokens': 7,\n",
    "    'temperature': 0,\n",
    "}\n",
    "\n",
    "completion_response = requests.post(f'http://{ip_addr}:8000/v1/completions', headers=vllm_headers, json=json_data)\n",
    "\n",
    "print(completion_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternatively, you can call this with the Open AI library, but requires that to be downloaded\n",
    "from openai import OpenAI\n",
    "\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = f\"http://{ip_addr}:8000/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "completion = client.completions.create(model=model_id,\n",
    "                                      prompt=\"San Francisco is a\")\n",
    "print(\"Completion result:\", completion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
