{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook we will use to find and deploy the most affordable GPU's on the market with the Shadeform API. We limit our search to machines with 1x80GB A100 machines, but that is easily configurable below.\n",
    "\n",
    "This notebook leverages the docker container created under `llm-eval-harness-benchmarking` to run basic huggingface benchmarks for a given huggingface model.\n",
    "\n",
    "It also re-uses code from [find_and_use_gpus.ipynb](link.com) for using the Shadeform API to find available instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://api.shadeform.ai/v1/instances\"\n",
    "instance_type_url = base_url + \"/types\"\n",
    "create_url = base_url + \"/create\"\n",
    "headers = {\n",
    "    \"X-API-KEY\": \"<add-your-key-here>\", \n",
    "    \"Content-Type\" : \"application/json\"\n",
    "}\n",
    "shade_instance_type = \"A6000\"\n",
    "gpu_type = \"A6000\"\n",
    "num_gpus = 1\n",
    "\n",
    "params = {\n",
    "    'gpu_type' : gpu_type,\n",
    "    'sort' : 'price',\n",
    "    'available' : True,\n",
    "    'num_gpus' : num_gpus\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.request(\"GET\", instance_type_url, headers=headers, params=params)\n",
    "instance_types = json.loads(response.text)[\"instance_types\"]\n",
    "best_instance = None\n",
    "region = None\n",
    "if len(instance_types) > 0:\n",
    "    best_instance = instance_types[0]\n",
    "    region = best_instance['availability'][0]['region']\n",
    "    print(f\"The cheapest {gpu_type} instance with {num_gpus} gpu(s) is:\", best_instance)\n",
    "else:\n",
    "    print(f\"No instances of type {gpu_type} instance with {num_gpus} gpu(s) found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"cloud\": best_instance[\"cloud\"],\n",
    "  \"region\": region,\n",
    "  \"shade_instance_type\": shade_instance_type,\n",
    "  \"shade_cloud\": True,\n",
    "  \"name\": \"cool_gpu_server\",\n",
    "  \"launch_configuration\": {\n",
    "    \"type\": \"docker\",\n",
    "    \"docker_configuration\": {\n",
    "      \"image\": \"shadeform/lm-eval-harness\",\n",
    "      \"envs\": [\n",
    "      \t{\n",
    "\t      \t\"name\": \"model\",\n",
    "\t      \t\"value\": \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "      \t},\n",
    "      \t{\n",
    "\t      \t\"name\": \"tasks\",\n",
    "\t      \t\"value\": \"hellaswag\"\n",
    "      \t},\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#request the best instance that is available\n",
    "response = requests.request(\"POST\", create_url, json=payload, headers=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#easy way to visually see if this request worked\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_response = requests.request(\"GET\", base_url, headers=headers)\n",
    "\n",
    "print(instance_response.text)\n",
    "instance = json.loads(instance_response.text)[\"instances\"][0]\n",
    "instance_status = instance['status']\n",
    "if instance_status == 'active':\n",
    "    print(f\"Instance is active with IP: {instance['ip']}\")\n",
    "else:\n",
    "    print(f\"Instance isn't yet active: {instance}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
