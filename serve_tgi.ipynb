{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://api.shadeform.ai/v1/instances\"\n",
    "instance_type_url = base_url + \"/types\"\n",
    "create_url = base_url + \"/create\"\n",
    "headers = {\n",
    "    \"X-API-KEY\": \"<Add-your-key-here>\", \n",
    "    \"Content-Type\" : \"application/json\"\n",
    "}\n",
    "shade_instance_type = \"A6000\"\n",
    "gpu_type = \"A6000\"\n",
    "num_gpus = 1\n",
    "\n",
    "params = {\n",
    "    'gpu_type' : gpu_type,\n",
    "    'sort' : 'price',\n",
    "    'available' : True,\n",
    "    'num_gpus' : num_gpus\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", instance_type_url, headers=headers, params=params)\n",
    "\n",
    "#We need to filter out any clouds that don't have the launch configuration supported, at the time of writing that is just runpod\n",
    "launch_config_not_supported = [\"runpod\"]\n",
    "\n",
    "instance_types = json.loads(response.text)[\"instance_types\"]\n",
    "instance_types = [x for x in instance_types if x['cloud'] not in launch_config_not_supported]\n",
    "best_instance = None\n",
    "region = None\n",
    "if len(instance_types) > 0:\n",
    "    best_instance = instance_types[0]\n",
    "    region = best_instance['availability'][0]['region']\n",
    "    print(f\"The cheapest {gpu_type} instance with {num_gpus} gpu(s) is:\", best_instance)\n",
    "else:\n",
    "    print(f\"No instances of type {gpu_type} instance with {num_gpus} gpu(s) found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "port = 8000\n",
    "\n",
    "#If the model you need requires authenticated access, paste your Hugging Face api key here\n",
    "huggingface_token = \"\"\n",
    "\n",
    "payload = {\n",
    "  \"cloud\": best_instance[\"cloud\"],\n",
    "  \"region\": region,\n",
    "  \"shade_instance_type\": shade_instance_type,\n",
    "  \"shade_cloud\": True,\n",
    "  \"name\": \"text_generation_inference_server\",\n",
    "  \"launch_configuration\": {\n",
    "    \"type\": \"docker\", \n",
    "    \"docker_configuration\": {\n",
    "      \"image\": \"ghcr.io/huggingface/text-generation-inference:1.4\",\n",
    "      \"args\": \"--model-id \" + model_id + f\" --port {port}\",\n",
    "      \"envs\": [],\n",
    "      \"port_mappings\": [\n",
    "        {\n",
    "          \"container_port\": 8000,\n",
    "          \"host_port\": 8000\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "#Add another environment variable to the payload by adding a json\n",
    "if huggingface_token != \"\":\n",
    "  token_env_json = {\n",
    "    \"name\": \"HUGGING_FACE_HUB_TOKEN\",\n",
    "    \"value\" : huggingface_token\n",
    "  }\n",
    "  payload[\"launch_configuration\"][\"docker_configuration\"][\"envs\"].append(token_env_json)\n",
    "\n",
    "\n",
    "print(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#request the best instance that is available\n",
    "response = requests.request(\"POST\", create_url, json=payload, headers=headers)\n",
    "#easy way to visually see if this request worked\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_response = requests.request(\"GET\", base_url, headers=headers)\n",
    "ip_addr = \"\"\n",
    "instance = json.loads(instance_response.text)[\"instances\"][0]\n",
    "instance_status = instance['status']\n",
    "if instance_status == 'active':\n",
    "    print(f\"Instance is active with IP: {instance['ip']}\")\n",
    "    ip_addr = instance['ip']\n",
    "else:\n",
    "    print(f\"Instance isn't yet active: {instance}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wait until the previous cell has an IP address associated with it, and then add a few minutes for the VLLM server to stand up. \n",
    "#It is usually best to look at the logs on the dashboard to tell when the model is loaded.\n",
    "\n",
    "\n",
    "tgi_headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "\n",
    "json_data = {\n",
    "    'model': model_id,\n",
    "    'prompt': 'New York City is the',\n",
    "    'max_tokens': 7,\n",
    "}\n",
    "\n",
    "completion_response = requests.post(f'http://{ip_addr}:{port}/v1/completions', headers=tgi_headers, json=json_data)\n",
    "\n",
    "print(completion_response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
